---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(pins)
library(tidymodels)
library(patchwork)
library(recipes)
library(embed)
library(corrplot)
library(dbscan)
source("preprocess_evaluate.R")

# Load the data
target_file = "team_summary"
df <- readRDS(paste0("clean_data/", target_file, ".rds")) %>% 
  dplyr::mutate(is_2020 = ifelse(SEASON_ID == "2020",  "is_2020", "not_2020")) %>% 
  dplyr::select(-file_name, -SEASON_ID) %>% 
  sample_n(1000)

# Set the default response column
RESPONSE = "is_2020"

# Columns to be intentionally excluded entirely, in all cases
dropped_columns = c(
  
)

# Potential response columns (will generally only consider one at a time)
response_columns = c(
  # "MARGIN",
  # "SQUAD_MARGIN",
  # "SCORE",
  # "GOAL",
  # "EXPECTED_SCORE",
  # "HOME_SCORE",
  # "AWAY_SCORE",
  # "SCORING_SHOTS",
  # # "SHOT_AT_GOAL",
  # "GOAL_ASSIST"
)

# Columns potentially removed in some cases (separated for simple commenting out)
screened_columns = c(
  
)

## not_considered_columns are any columns that will be removed
not_considered_columns = c(dropped_columns, screened_columns, response_columns)

## considered_columns will always include any remaining columns, and a special column "Y"
considered_columns = c(names(df)[!(names(df) %in% not_considered_columns)], "Y")
```

# Pre-processing (Isaac's code)
```{r}
X_total <- df %>% preprocess(response = RESPONSE, predictors = considered_columns)
X <- X_total$main

glimpse(X)
```

# XGBoost
```{r}
data_split <- initial_split(X, strata = "Y", p = 0.8)
train = training(data_split)
test = testing(data_split)


# xgb_spec <- boost_tree(
#   trees = 1000, 
#   tree_depth = tune(), min_n = tune(), 
#   loss_reduction = tune(),                     ## first three: model complexity
#   sample_size = tune(), mtry = tune(),         ## randomness
#   learn_rate = tune(),                         ## step size
# ) %>% 
#   set_engine("xgboost") %>% 
#   set_mode("classification")
# 
# 
# xgb_grid <- grid_latin_hypercube(
#   tree_depth(),
#   min_n(),
#   loss_reduction(),
#   sample_size = sample_prop(),
#   finalize(mtry(), data_split),
#   learn_rate(),
#   size = 30
# )
# 
# xgb_grid
# 
# xgb_wf <- workflow() %>%
#   add_formula(is_2020 ~ .) %>%
#   add_model(xgb_spec)
# 
# xgb_wf
```


```{r}
# train_x = train %>% select(-Y)
# train_y = train %>% pull(Y)
# 
xgb_model =  parsnip::boost_tree(
  mode = "classification",
  trees = 1000) %>%
  set_engine("xgboost", objective = "reg:squarederror")

xgb_recipe = recipes::recipe(Y ~ ., data = train) %>%
  step_knnimpute(all_predictors()) %>%
  prep()

xgb_wf = workflows::workflow() %>%
  add_model(xgb_model) %>%
  # add_recipe(xgb_recipe)
  add_formula(Y ~ .)

# xgb_model %>% fit_xy(x = train_x, y = train_y)
xgb_final = xgb_wf %>% fit(data = train)

last_fit(xgb_final, data_split)
```


```{r}
```